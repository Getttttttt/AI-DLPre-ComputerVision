# 一、引言

## 1.  简介：计算机视觉的定义和重要性

### 1.1 定义

​	计算机视觉是人工智能 (AI) 的一个领域，是指让计算机和系统能够从图像、视频和其他视觉输入中获取有意义的信息，并根据该信息采取行动或提供建议。 如果说人工智能赋予计算机思考的能力，那么计算机视觉就是赋予发现、观察和理解的能力。

### 1.2重要性

​	人的大脑皮层， 有差不多 70% 都是在处理视觉信息。 是人类获取信息最主要的渠道，没有之一。

​	在网络世界，照片和视频（图像的集合）也正在发生爆炸式的增长！

​	下图是网络上新增数据的占比趋势图。灰色是结构化数据，蓝色是非结构化数据（大部分都是图像和视频）。可以很明显的发现，图片和视频正在以指数级的速度在增长。

![图片和视频数据在飞速增长](https://easyai.tech/wp-content/uploads/2022/08/846a6-2020-03-05-increase.png)

​	而在计算机视觉出现之前，图像对于计算机来说是黑盒的状态。

​	一张图片对于机器只是一个文件。机器并不知道图片里的内容到底是什么，只知道这张图片是什么尺寸，多少MB，什么格式的。

如果计算机、人工智能想要在现实世界发挥重要作用，就必须看懂图片！这就是计算机视觉要解决的问题。



## 2. 历史简览：从早期研究到深度学习的革新

**1、20世纪50年代，主题是二维图像的分析和识别**

​	1959年，神经生理学家David Hubel和Torsten Wiesel通过猫的视觉实验，首次发现了视觉初级皮层神经元对于移动边缘刺激敏感，发现了视功能柱结构，为视觉神经研究奠定了基础——促成了计算机视觉技术40年后的突破性发展，奠定了深度学习之后的核心准则。

![img](https://pic4.zhimg.com/80/v2-198336fcd159146af1263f19c7483a4b_1440w.webp)

> 图片来源 [https://www.sohu.com/a/302537186_114877](https://link.zhihu.com/?target=https%3A//www.sohu.com/a/302537186_114877)

​	1959年，Russell和他的同学研制了一台可以把图片转化为被二进制机器所理解的灰度值的仪器——这是第一台数字图像扫描仪，处理数字图像开始成为可能。

​	这一时期，研究的主要对象如光学字符识别、工件表面、显微图片和航空图片的分析和解释等。

**2、20世纪60年代，开创了三维视觉理解为目的的研究**

​	1965年， Lawrence Roberts《三维固体的机器感知》描述了从二维图片中推导三维信息的过程。——现代计算机视觉的前导之一，开创了理解三维场景为目的的计算机视觉研究。他对积木世界的创造性研究给人们带来极大的启发，之后人们开始对积木世界进行深入的研究，从边缘的检测、角点特征的提取，到线条、平面、曲线等几何要素分析，到图像明暗、纹理、运动以及成像几何等，并建立了各种数据结构和推理规则。

![img](https://pic2.zhimg.com/80/v2-6291c7ebbe13d38f9cb749c78ff4774d_1440w.webp)

> 图片来源 [http://cs231n.stanford.edu/](https://link.zhihu.com/?target=http%3A//cs231n.stanford.edu/)

​	1966， MITAI实验室的Seymour Papert教授决定启动夏季视觉项目，并在几个月内解决机器视觉问题。Seymour和Gerald Sussman协调学生将设计一个可以自动执行背景/前景分割，并从真实世界的图像中提取非重叠物体的平台。——虽然未成功，但是计算机视觉作为一个科学领域的正式诞生的标志。

​	1969年秋天，贝尔实验室的两位科学家Willard S. Boyle和George E. Smith正忙于电荷耦合器件（CCD）的研发。它是一种将光子转化为电脉冲的器件，很快成为了高质量数字图像采集任务的新宠，逐渐应用于工业相机传感器，标志着计算机视觉走上应用舞台，投入到工业机器视觉中。

**3、20世纪70年代，出现课程和明确理论体系**

​	70年代中期，麻省理工学院（MIT）人工智能（AI）实验室：CSAIL正式开设计算机视觉课程。



![img](https://pic1.zhimg.com/80/v2-fd07bafc8fd56ad386d57b4d8a4a3db8_1440w.webp)

> 图片来源 [https://www.sohu.com/a/84165588_297710](https://link.zhihu.com/?target=https%3A//www.sohu.com/a/84165588_297710)

​	1977年David Marr在MIT的AI实验室提出了，计算机视觉理论（Computational Vision），这是与 Lawrence Roberts当初引领的积木世界分析方法截然不同的理论。计算机视觉理论成为80年代计算机视觉重要理论框架，使计算机视觉有了明确的体系，促进了计算机视觉的发展。

**4、20世纪80年代 ，独立学科形成，理论从实验室走向应用**

​	1980年，日本计算机科学家Kunihiko Fukushima在Hubel和Wiesel的研究启发下，建立了一个自组织的简单和复杂细胞的人工网络——Neocognitron，包括几个卷积层（通常是矩形的），他的感受野具有权重向量（称为滤波器）。这些滤波器的功能是在输入值的二维数组（例如图像像素）上滑动，并在执行某些计算后，产生激活事件（2维数组），这些事件将用作网络后续层的输入。Fukushima的Neocognitron可以说是第一个神经网络，是现代 CNN 网络中卷积层+池化层的最初范例及灵感来源。

​	1982年，David Marr发表了有影响的论文-“愿景：对人类表现和视觉信息处理的计算研究”。基于Hubel和Wiesel的想法视觉处理不是从整体对象开始, David介绍了一个视觉框架，其中检测边缘，曲线，角落等的低级算法被用作对视觉数据进行高级理解的铺垫。同年《视觉》（Marr, 1982）一书的问世，标志着计算机视觉成为了一门独立学科。

​	1982年 日本COGEX公司于生产的视觉系统DataMan，是世界第一套工业光学字符识别（OCR）系统。

​	1989年，法国的Yann LeCun将一种后向传播风格学习算法应用于Fukushima的卷积神经网络结构。在完成该项目几年后，LeCun发布了LeNet-5--这是第一个引入今天仍在CNN中使用的一些基本成分的现代网络。现在卷积神经网络已经是图像、语音和手写识别系统中的重要组成部分。

**5、20世纪90年代，特征对象识别开始成为重点**

​	1997年，伯克利教授Jitendra Malik（以及他的学生Jianbo Shi）发表了一篇论文，描述了他试图解决感性分组的问题。研究人员试图让机器使用图论算法将图像分割成合理的部分（自动确定图像上的哪些像素属于一起，并将物体与周围环境区分开来）

​	1999年， David Lowe 发表《基于局部尺度不变特征（SIFT特征）的物体识别》，标志着研究人员开始停止通过创建三维模型重建对象，而转向基于特征的对象识别。

![img](https://pic2.zhimg.com/80/v2-a95479b4dde8cae49e96b1ec2b9db135_1440w.webp)

> 图片来源 [https://wenku.baidu.com/view/152b0a9302020740be1e9b9e.html](https://link.zhihu.com/?target=https%3A//wenku.baidu.com/view/152b0a9302020740be1e9b9e.html)

​	1999年，Nvidia公司在推销Geforce 256芯片时，提出了GPU概念。GPU是专门为了执行复杂的数学和集合计算而设计的数据处理芯片。伴随着GPU发展应用，游戏行业、图形设计行业、视频行业发展也随之加速，出现了越来越多高画质游戏、高清图像和视频。

**6、21世纪初，图像特征工程,出现真正拥有标注的高质量数据集**

​	2001年，Paul Viola 和Michael Jones推出了第一个实时工作的人脸检测框架。虽然不是基于深度学习，但算法仍然具有深刻的学习风格，因为在处理图像时，通过一些特征可以帮助定位面部。该功能依赖于Viola / Jones算法，五年后，Fujitsu 发布了一款具有实时人脸检测功能的相机。

​	2005年，由Dalal & Triggs提出来方向梯度直方图，HOG（Histogramof Oriented Gradients）应用到行人检测上。是目前计算机视觉、模式识别领域很常用的一种描述图像局部纹理的特征方法。

​	2006年，Lazebnik, Schmid & Ponce提出一种利用空间金字塔即 SPM （Spatial Pyramid Matching）进行图像匹配、识别、分类的算法，是在不同分辨率上统计图像特征点分布，从而获取图像的局部信息。

​	2006年，Pascal VOC项目启动。它提供了用于对象分类的标准化数据集以及用于访问所述数据集和注释的一组工具。创始人在2006年至2012年期间举办了年度竞赛，该竞赛允许评估不同对象类识别方法的表现。检测效果不断提高。

​	2006年左右，Geoffrey Hilton和他的学生发明了用GPU来优化深度神经网络的工程方法，并发表在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。他给多层神经网络相关的学习方法赋予了一个新名词–“深度学习”。随后深度学习的研究大放异彩，广泛应用在了图像处理和语音识别领域，他的学生后来赢得了2012年ImageNet大赛，并使CNN家喻户晓。

​	2009年，由Felzenszwalb教授在提出基于HOG的deformable parts model(DPM)，可变形零件模型开发，它是深度学习之前最好的最成功的objectdetection & recognition算法。它最成功的应用就是检测行人，目前DPM已成为众多分类、分割、姿态估计等算法的核心部分，Felzenszwalb本人也因此被VOC授予"终身成就奖"。

![img](https://pic2.zhimg.com/80/v2-27dbf54f9a9a0db131618fa288a7d85d_1440w.webp)

> 图片来源 [http://www.360doc.com/content/14/0722/16/10724725_396297961.shtml](https://link.zhihu.com/?target=http%3A//www.360doc.com/content/14/0722/16/10724725_396297961.shtml)

**7、2010年-至今 深度学习在视觉中的流行，在应用上百花齐放**

​	2009年，李飞飞教授等在CVPR2009上发表了一篇名为《ImageNet: A Large-Scale Hierarchical Image Database》的论文，发布了ImageNet数据集，这是为了检测计算机视觉能否识别自然万物，回归机器学习，克服过拟合问题，经过三年多在筹划组建完成的一个大的数据集。从10年-17年，基于ImageNet数据集共进行了7届ImageNet挑战赛，李飞飞说"ImageNet改变了AI领域人们对数据集的认识，人们真正开始意识到它在研究中的地位，就像算法一样重要"。ImageNet是计算机视觉发展的重要推动者，和深度学习热潮的关键推动者，将目标检测算法推向了新的高度。

![img](https://pic2.zhimg.com/80/v2-d61843e9d089eae746c46aff69e2f631_1440w.webp)

> 图片来源 [http://cs231n.stanford.edu/](https://link.zhihu.com/?target=http%3A//cs231n.stanford.edu/)

​	2012 年，Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 创造了一个“大型的深度卷积神经网络”，也即现在众所周知的 AlexNet，赢得了当年的 ILSVRC。这是史上第一次有模型在 ImageNet 数据集表现如此出色。论文“ImageNet Classification with Deep Convolutional Networks”，迄今被引用约 7000 次，被业内普遍视为行业最重要的论文之一，真正展示了 CNN 的优点。机器识别的错误率从25%左右。降低了百分之16%左右，跟人类相比差别不大。是自那时起，CNN 才成了家喻户晓的名字。

![img](https://pic3.zhimg.com/80/v2-f3344631fc7d73618d97ba3f18701d42_1440w.webp)

![img](https://pic1.zhimg.com/80/v2-bf3867966fc46feac560ed35f269fedc_1440w.webp)

> 图片来源 [http://cs231n.stanford.edu/](https://link.zhihu.com/?target=http%3A//cs231n.stanford.edu/)

​	2014年，蒙特利尔大学提出生成对抗网络（GAN）：拥有两个相互竞争的神经网络可以使机器学习得更快。一个网络尝试模仿真实数据生成假的数据，而另一个网络则试图将假数据区分出来。随着时间的推移，两个网络都会得到训练，生成对抗网络（GAN）被认为是计算机视觉领域的重大突破。

​	2017-2018 年深度学习框架的开发发展到了成熟期。PyTorch 和 TensorFlow 已成为首选框架，它们都提供了针对多项任务（包括图像分类）的大量预训练模型。

​	近年来，国内外巨头纷纷布局计算机视觉领域，开设计算机视觉研究实验室。以计算机视觉新系统和技术赋能原有的业务，开拓战场。

​	如Facebook的AI Research（FAIR）在视觉方面2016年声称其DeepFace人脸识别算法有着97.35%的识别准确率，几乎与人类不分上下。2017，Lin, Tsung-Yi等提出特征金字塔网络，可以从深层特征图中捕获到更强的语义信息。同时提出Mask R-CNN，用于图像的实例分割任务，它使用简单、基础的网络设计，不需要多么复杂的训练优化过程及参数设置，就能够实现当前最佳的实例分割效果，并有很高的运行效率。

​	2016，亚马逊收购了一支欧洲顶级计算机视觉团队，为Prime Air无人机加上识别障碍和着陆区域的能力。开发无人机送货。2017年亚马逊网络服务（AWS）宣布对其识别服务进行了一系列更新，为云客户提供基于机器学习的计算机视觉功能。客户将能够在数百万张面孔的集合上进行实时人脸搜索。例如，Rekognition可用于验证一个人的图像与现有数据库中的另一个图像相匹配，数据库高达数千万个图像，具有亚秒级延迟。

​	2018年末，英伟达发布的视频到视频生成（Video-to-Video synthesis），它通过精心设计的发生器、鉴别器网络以及时空对抗物镜，合成高分辨率、照片级真实、时间一致的视频，实现了让AI更具物理意识，更强大，并能够推广到新的和看不见的更多场景。

​	2019， BigGAN，同样是一个GAN，只不过更强大，是拥有了更聪明的课程学习技巧的GAN，由它训练生成的图像连它自己都分辨不出真假，因为除非拿显微镜看，否则将无法判断该图像是否有任何问题，因而，它更被誉为史上最强的图像生成器。

​	2020年5月末，Facebook发布新购物AI，通用计算机视觉系统GrokNet让“一切皆可购买”。

**2021年**

​	遮盖图像建模、非Transformer架构、神经辐射场等技术快速发展，成为计算机视觉的热点研究领域；脉冲视觉领域发展，将开辟机器视觉新路线

**2022年**

**1. Transformer统治计算机视觉**

​	Transformer 模型在 2017 年在Attention is All You Need论文中被提出，之后广泛用于深度学习领域，为各种 NLP 任务设定了标准，并开创了大型语言模型 (LLM) 的时代。Vision Transformer (ViT) 于 2020 年底推出，标志着这些基于自我注意的模型在计算机视觉环境中的首次应用。

​	2022年，研究将 Transformer 模型推向了计算机视觉的前沿，在各种任务上实现了最先进的性能。例如：DETR、SegFormer、Swin Transformer和ViT。这个GitHub总结了相当于的基于Transformer的网络。

https://github.com/Yangzhangcst/Transformer-in-Computer-Vision

**2. 以数据为中心的计算机视觉获得牵引力**

​	随着计算机视觉的成熟，越来越多的机器学习开发工作专注于整理、清理和扩充数据。数据质量正在成为性能的瓶颈，行业正朝着数据模型协同设计的方向发展。以数据为中心的机器学习会越来越受欢迎。

​	这一努力的掌舵者是新一波的初创公司——合成数据生成公司（gretel、Datagen、Tonic）和评估、可观察性和实验跟踪工具（Voxel51、Weights & Biases 、CleanLab）——加入现有的标签和注释服务（Labelbox、Label Studio、CVAT、Scale、V7 ) 的努力。

**3. AI 生成的艺术作品**

​	在生成对抗网络 (GAN) 的改进和传播模型的快速发展和迭代之间，AI生成的艺术正在经历只能被描述为复兴的过程。借助Stable Diffusion、Nightcafe、Midjourney和 OpenAI的DALL-E2等工具，现在可以根据用户输入的文本提示生成令人难以置信的细微图像。Artbreeder允许用户将多个图像“培育”成新作品，Meta的Make-A-Video从文本生成视频，而RunwayML在创建动画和编辑视频方面改变了游戏规则。其中许多工具还支持修复和outpainting，可用于编辑和扩展图像范围。

​	随着所有这些工具彻底改变 AI 艺术能力，争议几乎是不可避免的，而且已经有很多。9 月，一张AI 生成的图像赢得了一场美术比赛，引发了关于什么才算是艺术，以及所有权、归属和版权如何适用于这类新内容的激烈讨论。估计这个讨论会越来约激烈。

**4. 多模态人工智能成熟**

​	除了 AI 生成的艺术作品，2022 年还见证了多种模式交叉领域的大量研究和应用。处理多种类型数据（包括语言、音频和视觉）的模型和管道正变得越来越流行。这些学科之间的界限从未如此模糊，异花授粉也从未如此富有成果。

​	这种上下文冲突的核心是对比学习，它改进了将多种类型的数据嵌入同一空间的方法，开创性的例子是 Open AI 的对比语言-图像预训练 ( CLIP ) 模型。

​	这样做的一个结果是能够根据文本或其他图像的输入对图像集进行语义搜索。这刺激了矢量搜索引擎的繁荣，Qdrant、Pinecone、Weaviate、Milvus和其他引擎引领潮流。同样，模态之间的系统连接正在加强视觉问答和零镜头和少镜头图像分类。

**2023年：**

**NO1**. YOLO重生：NextGen 目标检测

​	在过去十年的大部分时间里，YOLO 系列模型一直是实时目标检测任务的热门选择。在 2023 年之前，YOLO 已经经历了多次迭代，流行的变体包括 Ultralytics 的 YOLOv5 和 YOLOv8（2022 年 12 月）、美团的 YOLOv6 和 YOLOv7。

​	2023年5月，Deci AI发布了YOLO-NAS，这是一个在NAS的帮助下创建的YOLO风格模型。该模型比以前的 YOLO 模型更快、更准确，并且对量化有很强的支持。最小的量化变体在仅 2.36ms 的延迟下实现了 47.03 的平均精度 （mAP）！YOLO-NAS 还构成了 Deci 最先进的 （SOTA） 姿态估计模型 YOLO-NAS-Pose 的基础。



**NO2**. Segment Anything：图像分割基础模型

​	Meta AI Research 的 SAM模型 可以说是计算机视觉中分割任务的第一个基础模型。过去，如果要为数据生成高质量的像素级分类预测，则需要从头开始训练分割模型。

​	SAM的出现彻底改变了游戏规则。现在可以通过使用边界框或正/负关键点提示模型来分割图像中的所有内容，或实例分割图像中的对象。GitHub 存储库目前有 40k 颗星，而且还在持续增加！

​	SAM 和与该模型共同开发的 11 亿个掩码数据集已经催生了大量令人难以置信的相关项目，包括：

更智能的图像分割模型: FastSAM, MobileSAM, NanoSAM, EdgeSAM

合成式应用: Recognize Anything, Inpaint Anything, Track-Anything, Grounded-Segment-Anything

3D图像分割应用: Segment Anything in 3D with NeRFs, Segment Anything 3D

医疗图像分割模型: MedSAM, SAM-Med3D

**NO3.** DINOv2：来自自监督学习的SOTA模型 

​	自然语言处理应用中的一种标准技术是自监督学习，其中模型是根据输入数据本身生成的信号进行训练的，而不是预先指定的label。例如，在预训练中，可以训练模型来预测文本序列中LLM下一个标记。像这样的自我监督方法有助于减少对人类注释数据的依赖.

​	在计算机视觉的背景下，对比学习（参见 CLIP）等方法严重依赖人类提供的标题和元数据，将模型的理解限制在标题的质量和注释图像的多样性上。DINOv2 通过对视觉任务应用自我监督方法克服了这些限制。

​	当在一组不同的 142M 图像上进行预训练并结合基本任务特定的头部时，Meta 的 DINOv2 骨干在从深度估计到语义分割的各种视觉任务中实现了最先进的性能。更重要的是，DINOv2 方法为任何人提供了一个模板，可以用很少的标记图像来训练高质量的模型！

**NO4.** 3D Gaussian Splatting 

​	将 3D Gaussian Splatting与其他用于视图合成的竞争技术进行比较，展示了在训练时间、延迟和准确性方面的优势。2023 年上半年，NeRF 主导了关于新观点综合的讨论。正如我们在 5 月份所记录的那样，在 CVPR 之前，“NeRF”一词在 CVPR 2023 论文标题中的出现频率比 CVPR 2022 论文标题中的频率高 80%

​	2023 年下半年出现了一种称为Gaussian Splatting的替代方法，它代表了具有 3 维（或更高）高斯的场景。光栅化技术可实现SOTA视觉质量和实时（>100 fps）渲染。与 NeRF 相比，Gaussian Splatting还可以提高训练的速度。

**NO5.** 文本转图像模型 

​	2022 年，DALL-E 2 被《时代》杂志评为年度 100 项发明之一，Midjourney 推出了 v1版本的Stable Diffusion ，为文本到图像 （T2I） 模型铺平了道路。但结果喜忧参半——有六根手指的手、不受欢迎的空间构图和不令人满意的美学特征都很常见。更重要的是，图像生成推理时间可能很长，从而减慢了实验速度。

​	2023年，T2I型号取得了巨大的飞跃。Midjourney 的创作已经栩栩如生;DALL-E 3 优化文本提示;Stable Diffusion XL 生成逼真的面孔和清晰的文本;Imagen 2 允许用户在 AI 生成的图像中添加不可见的水印。

**NO6.** ControlNet

​	2023 年 T2I 模型的主要生成建模技术是扩散模型。在图像生成的背景下，扩散模型的任务是迭代地将嘈杂的初始图像转换为连贯的低噪声图像。这种技术非常强大，但在真空中，仅通过文本提示对最终生成的图像进行控制可能既不精确又笨拙。

​	ControlNet 可以更大程度地控制 T2I 扩散模型输出的组成和样式。ControlNet可以明确地控制生成图像中对象的轮廓，包括 Canny 边缘贴图或涂鸦、生成的人的姿势等等。

**NO7.** LoRA

​	LoRA 最初是为LLMs开发的一种参数高效微调技术，它可以使现有模型的调整变得简单、实惠且易于访问。该方法的工作原理是在基础模型中插入小的低秩矩阵，并在微调数据上学习这些权重的良好配置，同时保持原始模型的权重固定。

​	那么LoRA 在计算机视觉中的主要应用是微调扩散模型以匹配某些风格，从像素艺术到体素。Hugging Face 上甚至还有一个 LoRA 微调库！LoRA 模型还用于将潜在一致性模型的推理步骤减少到稳定扩散 （LCM-LoRA）

​	但 LoRA 模型也适用于其他视觉环境，从语义分割到 DreamBooth 微调。LoRA 的一个特别有趣的应用是 DreamSim，该技术用于学习 SOTA 人类视觉相似度指标。

**NO8.** Ego-Exo4D：视频感知的基础数据集

​	过去的两年里，Meta AI 和 15 所大学研究人员共同努力，收集了迄今为止最大、最高质量的数据集，用于视频学习和多模态感知。这个 Ego-Exo4D 数据集包含 1,400+ 参与者的 800 小时镜头，这些参与者执行基于技能的人类活动，从烹饪到跳舞，并有可能影响人类和机器人学习和获取技能的方式。

​	对于每个场景，数据集都包含第一人称（以自我为中心）视角的同步视频片段，使用 Meta 的 Aria 眼镜和第三人称（外中心）视角拍摄。这些视频数据通过第一人称叙述、第三人称播放以及 3D 身体和手部姿势估计等任务的注释进行了增强。结合数据集，Meta 提供了一个基准测试套件，并计划在 2024 年举办基准测试挑战

**NO9.** T2V

​	AI 创意工具巨头 Runway 一直处于领先地位，发布了其文本到视频 （T2V） 模型的 Gen-1 和 Gen-2，以及用于帧插值和从蒙版区域生成运动的工具。但 Runway 远非 T2V 游戏中的唯一参与者：11 月，Pika Labs 宣布了他们的“创意到视频”平台和 5500 万美元的融资，Meta 宣布了他们的 SOTA 模型 Emu Video，它将 T2V 任务分为 （i） 文本条件图像生成，以及 （ii） 基于图像和文本提示的视频生成。

还值得一提的是 2023 年推出的几款开源 T2V 模型：

ModelScope’s Text-to-Video

Text2Video-Zero from PicsArt

VideoCrafter1: Open Diffusion Models for High-Quality Video Generation

AnimateZero: Video Diffusion Models are Zero-Shot Image Animators

Text-to-video model in Hugging Face Diffusers library

Synthesia: T2V platform for avatars

**NO10.** 多模态 LLMs 

​	2023是大模型的元年，很开心大模型也开始走向多模态大模型，这将赋予大模型更多的能力，一步步走向AGI。

​	多模态 LLMs （MLLM） 通过提供LLM接受多个模态的数据作为输入的能力来弥合这种模态差距。在大多数情况下，预训练LLM通过适配器连接到视觉模块，其权重通过多模态任务（如图像-文本匹配或对比学习）进行调整。

​	最激动的多模态大模型莫过于是 OpenAI 的 GPT-4 Vision 和 Google DeepMind 的 Gemini。其他值得注意的（和开源的）多模态LLMs包括 LLaVA、CogVLM、InstructBLIP、Fuyu-8B 和 IDEFICS。

**NO11.** LLM-辅助视觉推理 

​	ViperGPT 的插图将一般推理能力LLMs与专家视觉模型相结合，以回答视觉问题。视频最初来自 ViperGPT 项目页面。

​	弥合模态差距的另一种方法是使用LLMs推理引擎，并允许它们调用视觉模型。这种方法解开了多模态任务中通常存在的视觉理解和逻辑推理，减轻了视觉模型的负担。

​	LLMs可以充当推理引擎，确定需要执行哪些特定的视觉任务，将这些任务的执行委托给专家模型，并根据这些模型的输出得出结论。这种方法本质上是模块化的（可以添加或替换视觉模型），并且更具可解释性（故障可以追溯到特定的推理步骤）。

​	2023 年，我们看到了多个符合这种模式的病毒式项目，包括 Chameleon、HuggingGPT、VisProg 和 ViperGPT。其中 ViperGPT 为零样本视觉问答和接地问答任务设置了新的 SOTA！

**2024**

Sora





## 3. 有趣的演示（Demo）

（1）上次提到的蒙娜丽莎*：https://humanaigc.github.io/emote-portrait-alive/ 。

（2）[InstantID：一张照片，无需训练，秒级个人写真生成 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/679714372)（有开源demo）



# 二、未来发展

## 1. 多模态学习

## 1.1定义：结合视觉、语言和声音等多种模态的学习

​	多模态机器学习，英文全称 MultiModal Machine Learning (MMML)

​	模态（modal）是事情经历和发生的方式，我们生活在一个由多种模态（Multimodal）信息构成的世界，包括视觉信息、听觉信息、文本信息、嗅觉信息等等，当研究的问题或者数据集包含多种这样的模态信息时我们称之为多模态问题，研究多模态问题是推动人工智能更好的了解和认知我们周围世界的关键。

###  模态

​	**模态是指一些表达或感知事物的方式，每一种信息的来源或者形式，都可以称为一种模态**。例如，人有触觉，听觉，视觉，嗅觉；信息的媒介，有语音、视频、文字等；多种多样的传感器，如雷达、红外、加速度计等。以上的每一种都可以称为一种模态。

**	相较于图像、语音、文本等多媒体(Multi-media)数据划分形式，“模态”是一个更为细粒度的概念，同一媒介下可存在不同的模态。** 比如我们可以把两种不同的语言当做是两种模态，甚至在两种不同情况下采集到的数据集，亦可认为是两种模态。

### 多模态

​	**多模态即是从多个模态表达或感知事物。** 多模态可归类为同质性的模态，例如从两台相机中分别拍摄的图片，异质性的模态，例如图片与文本语言的关系。

**多模态可能有以下三种形式：**

\- **描述同一对象的多媒体数据**。如互联网环境下描述某一特定对象的视频、图片、语音、文本等信息。下图即为典型的多模态信息形式。

![img](https://pic3.zhimg.com/80/v2-0f9181a9b97891fab9a4ba4ab55a54f2_1440w.webp)

“下雪”场景的多模态数据(图像、音频与文本)

\- **来自不同传感器的同一类媒体数据**。如医学影像学中不同的检查设备所产生的图像数据， 包括B超(B-Scan ultrasonography)、计算机断层扫描(CT)、核磁共振等；物联网背景下不同传感器所检测到的同一对象数据等。

\- **具有不同的数据结构特点、表示形式的表意符号与信息**。如描述同一对象的结构化、非结构化的数据单元；描述同一数学概念的公式、逻辑 符号、函数图及解释性文本；描述同一语义的词向量、词袋、知识图谱以及其它语义符号单元等。

通常主要研究模态包括"**3V**"：即**Verbal(文本)、Vocal(语音)、Visual(视觉)**。 人跟人交流时的多模态：

![img](https://pic4.zhimg.com/80/v2-225d569ddd4d0427b54c44d34aa6b18f_1440w.webp)

multimodal communicative behaviors



### 多模态学习

​	**多模态机器学习**是从多种模态的数据中学习并且提升自身的算法，它不是某一个具体的算法，它是一类算法的总称。

​	从**语义感知**的角度切入，多模态数据涉及**不同的感知通道**如视觉、听觉、触觉、嗅觉所接收到的信息;在**数据层面**理解，多模态数据则可被看作**多种数据类型**的组合，如图片、数值、文本、符号、音频、时间序列，或者集合、树、图等不同数据结构所组成的复合数据形式，乃至来自不同数据库、不同知识库的各种信息资源的组合。**对多源异构数据的挖掘分析可被理解为多模态学习**。



![img](https://pic4.zhimg.com/80/v2-779aef8e97481d9cf2aa0dcc5f6e005b_1440w.webp)

## 1.2 技术原理：融合不同类型数据的深度学习模型

​        多模态学习的主要目标是学习如何从不同模态的数据中提取有用信息，并将这些信息融合到一个统一的表示中。为了实现这一目标，我们需要考虑以下几个方面：

（1）数据预处理：在进行多模态学习之前，我们需要对不同模态的数据进行预处理，以确保它们可以被模型所处理。这可能包括对图像进行缩放、旋转等操作，对文本进行分词、标记等操作。

（2）特征提取：在进行多模态学习时，我们需要从不同模态的数据中提取特征。这可以通过使用不同类型的特征提取器来实现，例如，对于图像，我们可以使用卷积神经网络(CNN)来提取特征；对于文本，我们可以使用循环神经网络(RNN)或者Transformer来提取特征。

（3）融合策略：在进行多模态学习时，我们需要考虑如何将不同模态的特征融合在一起。这可以通过使用不同类型的融合策略来实现，例如，我们可以使用加权平均、乘法融合等策略。

（4）学习算法：在进行多模态学习时，我们需要选择合适的学习算法。这可以包括监督学习、无监督学习、半监督学习等。

#### 核心算法原理和具体操作步骤以及数学模型公式详细讲解

##### 卷积神经网络(CNN)

卷积神经网络(CNN)是一种深度学习算法，主要用于图像分类和识别任务。它的核心思想是通过使用卷积层来提取图像的特征。

###### 卷积层的原理

卷积层是CNN的核心组件，它通过使用滤波器(也称为卷积核)来对输入图像进行卷积操作。滤波器可以看作是一个小的、有权重的矩阵，它会在输入图像上滑动，并对每个位置进行乘积运算。最终，我们可以得到一个和输入图像大小相同的输出图像，这个输出图像包含了图像中的特征信息。

###### 卷积层的具体操作步骤

定义滤波器：首先，我们需要定义一个滤波器，这个滤波器可以是任意形状和大小的，但通常我们使用较小的滤波器，例如3x3或5x5。

滑动滤波器：接下来，我们需要将滤波器滑动到输入图像上，并对每个位置进行乘积运算。这个过程称为滑动滤波器。

计算输出图像：最后，我们需要计算滑动滤波器后的输出图像，这个输出图像包含了图像中的特征信息。

###### 卷积层的数学模型公式

在本节中，我们将详细讲解卷积层的数学模型公式。

$$ y(i,j) = \sum{p=0}^{P-1} \sum{q=0}^{Q-1} x(i+p,j+q) \cdot w(p,q) $$

其中，$y(i,j)$ 表示输出图像的值，$x(i,j)$ 表示输入图像的值，$w(p,q)$ 表示滤波器的值，$P$ 和 $Q$ 分别表示滤波器的行数和列数。

##### 循环神经网络(RNN)

循环神经网络(RNN)是一种递归神经网络，主要用于处理序列数据，例如文本、音频等。它的核心思想是通过使用隐藏状态来捕捉序列中的长距离依赖关系。

######  RNN的具体操作步骤

初始化隐藏状态：首先，我们需要初始化隐藏状态，这个隐藏状态会在每个时间步骤中被更新。

计算输出：接下来，我们需要计算当前时间步骤的输出，这个输出会被用于下一个时间步骤的计算。

更新隐藏状态：最后，我们需要更新隐藏状态，这个隐藏状态会被用于下一个时间步骤的计算。

###### RNN的数学模型公式

在本节中，我们将详细讲解RNN的数学模型公式。

$$ ht = tanh(Wxt + Uh_{t-1} + b) $$

$$ yt = Wyht + by $$

其中，$ht$ 表示隐藏状态，$xt$ 表示输入向量，$yt$ 表示输出向量，$W$ 表示权重矩阵，$U$ 表示递归权重矩阵，$b$ 表示偏置向量，$Wy$ 表示输出权重矩阵，$b_y$ 表示输出偏置向量。

##### 注意力机制

注意力机制是一种用于计算输入序列中元素之间相对重要性的方法，它可以用于计算多模态数据中的权重。

###### 注意力机制的具体操作步骤

计算查询向量：首先，我们需要计算查询向量，这个查询向量会被用于计算权重。

计算键向量：接下来，我们需要计算键向量，这个键向量会被用于计算权重。

计算值向量：最后，我们需要计算值向量，这个值向量会被用于计算权重。

计算权重：最后，我们需要计算权重，这个权重会被用于计算输出。

######  注意力机制的数学模型公式

![image-20240309223959649](C:\Users\woolyy\AppData\Roaming\Typora\typora-user-images\image-20240309223959649.png)

#### 具体代码实例和详细解释说明

##### 图像和文本的多模态学习

在本例中，我们将使用Python和TensorFlow来实现图像和文本的多模态学习。首先，我们需要使用CNN来提取图像的特征，然后使用RNN来提取文本的特征，最后使用注意力机制来将这两种特征融合在一起。

###### 图像特征提取

首先，我们需要使用CNN来提取图像的特征。我们可以使用TensorFlow的Keras库来构建一个简单的CNN模型，如下所示：

```

model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3))) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(128, (3, 3), activation='relu')) model.add(MaxPooling2D((2, 2))) model.add(Flatten()) model.add(Dense(512, activation='relu')) model.add(Dense(10, activation='softmax')) ```
```

###### 文本特征提取

接下来，我们需要使用RNN来提取文本的特征。我们可以使用TensorFlow的Keras库来构建一个简单的RNN模型，如下所示：

```

model = Sequential() model.add(Embedding(inputdim=10000, outputdim=64, input_length=50)) model.add(LSTM(64)) model.add(Dense(10, activation='softmax')) ```
```

###### 融合策略

最后，我们需要使用注意力机制来将图像和文本的特征融合在一起。我们可以使用TensorFlow的Keras库来构建一个简单的注意力机制模型，如下所示：

```

inputimage = Input(shape=(224, 224, 3)) inputtext = Input(shape=(50,))

imagefeatures = model(inputimage) textfeatures = model(inputtext)

attentionweights = Dot(axes=1)([imagefeatures, textfeatures]) attentionweights = Softmax()(attention_weights)

fusedfeatures = Dot(axes=1)([imagefeatures, attentionweights]) fusedfeatures = Add()([fusedfeatures, textfeatures])

output = Dense(10, activation='softmax')(fused_features)

model = Model(inputs=[inputimage, inputtext], outputs=output) ```

```

训练和测试
最后，我们需要训练和测试我们的多模态学习模型。我们可以使用TensorFlow的Keras库来训练和测试我们的模型，如下所示：

```
```python model.compile(optimizer='adam', loss='categoricalcrossentropy', metrics=['accuracy']) model.fit([imagedata, textdata], labels, epochs=10, batchsize=32)
testloss, testaccuracy = model.evaluate([testimagedata, testtextdata], test_labels) ```

```



## 1.3 前沿Demo：自动图像字幕生成，多模态情感分析

多模态基础感知大模型APE [CVPR 2024 | 多模态基础感知大模型APE发布！在160种测试集上取得强力结果！-CSDN博客](https://blog.csdn.net/amusi1994/article/details/136467806)



## 2. 增强现实（AR）和虚拟现实（VR）

## 2.1 技术发展：从计算机视觉到沉浸式体验

​	VR是一种计算机生成的环境，它可以为用户提供一种身临其境的感觉，仿佛置身于一个完全由计算机生成的世界中。通过头戴式设备，用户可以在这个虚拟的世界中进行互动，体验前所未有的沉浸感。VR的主要特点是它创造了一个完全数字化的环境，让用户可以完全沉浸在计算机生成的世界中。

​	与VR不同，AR，即增强现实，侧重于将虚拟信息与现实世界相结合。AR技术通过将虚拟物体、图像或信息叠加到真实世界中，为用户提供了一种增强的现实体验。用户可以在现实世界中看到和交互虚拟元素，从而获得更多的信息或更好的体验。例如，通过AR技术，用户可以在购买之前预览家具在家中的摆放效果，或者在现实世界中看到虚拟的导航指示。

​	在实际应用中，VR和AR各有千秋。VR主要侧重于娱乐、[游戏](https://cloud.baidu.com/solution/game.html)、模拟训练等领域，它能为用户提供一个完全沉浸式的环境，让用户完全投入到计算机生成的世界中。而AR则更注重实用性和便利性，它可以为用户提供实时、交互式的增强信息，帮助用户更好地理解和操作现实世界。





#### **[增强现实技术](https://www.zhihu.com/search?q=增强现实技术&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A191543111})流程**

​	按照Ronald Azuma在1997年的总结，增强现实系统一般具有三个主要特征：虚实结合，实时交互，和三维配准（又称注册、匹配或对准）。近二十年过去了，AR已经有了长足的发展，系统实现的重心和难点也随之变化，但是这三个要素基本上还是AR系统中不可或缺的。

![img](https://pica.zhimg.com/80/v2-a3d1b1e5d27fc2eaa90d5f8bdc87e8f1_1440w.webp?source=1def8aca)

​	上图描绘了一个典型的[AR系统](https://www.zhihu.com/search?q=AR系统&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A191543111})的概念流程。从真实世界出发，经过数字成像，然后系统通过影像数据和传感器数据一起对三维世界进行感知理解，同时得到对三维交互的理解。3D交互理解的目的是告知系统要“增强”的内容。

​	在AR的技术流程中，中间的对于环境和交互的精准理解就是目前的瓶颈了。上图中间的基于多模态（简单说就是影像+传感器）的环境和交互理解，是两个充满了各种或明或暗的坑的领域，足以让很多假的猛士知难而退。







## 2.2 应用案例：游戏，教育，零售

**1.** **外科手术培训**

![img](https://pic2.zhimg.com/80/v2-2794d68872f70685497052efc6ddadb5_1440w.webp)

​	传统的外科手术培训，由于解刨标本稀缺，通常十几个人甚至几十个人，共用一个解刨标本进行培训练习，导致教学质量大受影响。

​	VR/AR技术提供的仿真3D模型，其便携性、易用性以及随时可操作练习，为外科手术培训提供了新的可能。据加州大学洛杉矶分校进行过一次随机调查显示，使用VR技术（由Osso提供的VR外科手术训练平台）进行外科手术培训，相对传统教学而言，手术完成速度快20%，手术完成度高38%。

**2.** **沉浸式心理治疗**

![动图封面](https://pic4.zhimg.com/v2-bb98b9a814782ba417b454d22ae05de7_b.jpg)



​	对于精神疾病的传统治疗方案，通常是引导与人交流并配合药物，达到疏解病人心理压力的治疗作用。但是多数精神病患者大多数时间是独处的，抗拒与人接触。VR/AR技术可以帮助患者在虚拟世界的互动中改善心理状况，可以塑造各种情景，让患者更快的进入情景，医生更快的掌握病情，更好更快的对症治疗。

​	比如在治疗社交焦虑障碍症时，常用的传统治疗方案之一就需要引导患者自行想象场景，这种主观性的方法由于不可控，治疗效果不甚理想。而VR/AR技术可以让患者走进医生设定的虚拟场景，在虚拟世界中克服自己的心理障碍。

​	同样的治疗方式也适用于PTSD患者，通过VR/AR技术提供的自然景象，能帮助他们更加放松。据调查，可以减少PTSD患者约20%的压力值。

**3.** **真实体验式看房**

![img](https://pic3.zhimg.com/80/v2-fa3d7cd8a01b5007735496d360877572_1440w.webp)

​	传统的看房，潜在买家往往需要经过漫长而复杂的调研和参观房屋的过程，而卖家或者中介往往也要配合买家时间协调看房，导致买卖双方都比较麻烦。

​	通过VR/AR技术实现数字化转型，帮助买卖双方更有效、更轻松的完成交易。VR/AR看房具备的优势，替代传统的营销方式，买卖双方无需协调时间，客户随时都可以进行虚拟看房，由于不受地域限制，房产买卖不再局限与本地客户，全球买家都可以参观房产，除此之外，VR/AR技术提供的互动性、娱乐性和个性化，增强客户体验，大幅提高投资回报率。

**4.** **技术员工技能培训**

![img](https://pic2.zhimg.com/80/v2-18177018bf925d148c36944f5a9f2af1_1440w.webp)

​	一名普通员工成为技能熟练的员工需要长时间的培训和不断练习，由于学习难度大，学习进度慢，加上越来越多的老员工接近退休年龄，技能专业型员工的稀缺成为很多企业亟待解决的问题。

​	Interplay Learning公司提供的在线培训平台，利用VR技术和3D模拟技术，可以更高效对员工进行培训，生动形象易于理解，帮助培训员工可以更快速的学习上手，提高培训效率。

**5.** **重型机械操作培训**

![img](https://pic4.zhimg.com/80/v2-44437caea1220660ba4760c71d020d3f_1440w.webp)

​	传统的重型机械操作培训为真机演练，要求学员到设备的操作场地，亲身登上设备座舱，在专业教练员的陪同下进行多种工况的模拟训练，这对培训设备投入、培训人员投入、培训场地投入都要求极高，导致培训难度大，培训效率低，培训训周期长。

​	AR技术与重型机械的结合，解决了传统的重型机械操作培训的难题。一家提供数字孪生解决方案的公司——DataMesh，为某重型机械制造企业提供的设备仿真操作培训/考试平台，利用AR技术、力学仿真技术、数字孪生技术，对重型机械进行动态操作模拟，经估算可降低挖掘机、旋挖钻机培训成本46%，缩短约两周的培训周期。

​	以下链接可以下载试用DataMesh提供的标准化产品：

[DataMesh|商询科技-数字孪生服务商 | DataMeshwww.datamesh.com.cn/go/zhihu/](https://link.zhihu.com/?target=https%3A//www.datamesh.com.cn/go/zhihu/)

**6.** **石油及天然气行业培训**

![img](https://pic3.zhimg.com/80/v2-a2e0388d34a2520ed507046a37f85fca_1440w.webp)

​	石油和天然气企业中，员工通常需要在现场和钻井平台进行实践练习，不仅耗时费力，培训成本高，尤其对于相对偏远的油气开采区，而且这种现场培训对没有经验的人来说是极其危险的。

​	一家叫Jasoren的公司提供的石油和天然气VR培训方案，通过引入VR技术，员工可以在多个虚拟场景中进行实操，降低公司培训成本，减少因设备操作不当造成的损害，同时减少危险的发生，另外还可以模拟危险场景进行练习（比如化学爆炸和火灾等难以在真实环境中复制的场景），通过场景演练提高培训人员在高压环境的决策能力。

**7.** **汽车维修**

![img](https://pic1.zhimg.com/80/v2-b1b4dfea52ca8f6b30a7c77c59193ce4_1440w.webp)

​	汽车维修领域的专家少，而且很难快速调动，导致他们受限于相对固定或者说范围较小的维修站点。AR技术应用于汽车维修，可以让汽修专家快速获取车辆的故障信息并进行实时的远程支持，最大限度的利用专家技能，从而减少维修时间，提高维修效率，提高客户满意度。

​	宝马公司开发的TIS 2.0平台，提供线上支持与辅助，通过佩戴的TSARAVision智能眼镜，服务中心的技术人员可以通过免提视频直接与专家联系，从而更快，更高效地解决问题。专家可以将技术指示和示意图投影到现场技术人员的眼镜显示屏上，还可以根据现场情况进行屏幕截图和放大图像，做出进一步指示。据宝马估计，该平台提供的维修速度相对以往而言提高70%到75%。

**8.** **服务业培训**

![img](https://pic1.zhimg.com/80/v2-74e2412ea07b5aacddfd929c779adf80_1440w.webp)

​	服务业人员通常要进行培训后才上岗，由于业务不熟练，以及真实场景的不确定性，面对不那么耐心的客户时，由于压力倍增，服务态度很有可能大打折扣，长此以往更有可能造成恶性循环，由此带来的人才流失，造成企业培训成本的增加。利用AR技术进行模拟服务场景的高压演练，可以自行设定各种意外情形，一方面帮助服务业人员更快更好的熟悉业务，另一方面可以帮助他们更好的应对未来可能面临的突发情况，帮助服务人员胜任工作，减少企业人才损失。

**9.** **军事训练**

![img](https://pic4.zhimg.com/80/v2-1e6e19ff7efc04bf5e920c707a317613_1440w.webp)

​	AR军事演练与传统的军事演练相比，成本更低，更安全，没有死亡或受伤的风险。

​	以军事训练中最昂贵的一项投入——“飞行演练”来说，一架飞机的成本动辄数百万甚至上千万美元，在只能有限使用的情形下，要想达到熟练的程度过程相当的漫长。而AR技术提供的飞行模拟器，可以进行反复演练，还可以增加训练场景中的危险元素，比如一群鸟从飞机前飞过，发动机出现了故障等，通过模拟场景更快地掌握飞行技能并模拟应对意外事件，相比真实演练收益更高。

​	除此之外，还有战场作战模拟、爆炸性危险物处理、战地医学反应模拟等，通过模拟各种高危险、强破坏力和极端条件的情形，不仅保障安全，还能达到显著的训练效果。

**10.** **寓教于乐**

![img](https://pic3.zhimg.com/80/v2-97cd68f33caa74d00278e7602b6e71aa_1440w.webp)

​	VR/AR技术应用于教学，将教育与娱乐相结合，让传统的课程更具吸引力，可以为学生提供与教学主题有关的信息，让晦涩难懂的知识点易于理解，比如个人的简介，有趣的事实，相关的历史数据，可视化的3D模型，很好的扩展了学生知识面，增加参与度、趣味性和互动性，改善学习效果。莱里亚理工学院将AR融入数学课程，广受学生好评，AR让数学课程也能轻松有趣。





## 2.3 前沿Demo：AR购物体验，VR手术模拟

[Unity开源项目：AR Foundation Demos - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/460594288)（有开源demo）





# 三、展望

## 1. 计算机视觉的社会影响

(1)计算机视觉极大地便利了我们的工作、生活。

​        计算机视觉作为人工智能领域的重要分支，已经渗透到我们生活的方方面面。从无人驾驶汽车到生产线上的质量检测，从医学诊断到人脸识别，计算机视觉正在改变我们的世界。
​	首先，让我们来看看计算机视觉在自动驾驶中的应用。自动驾驶汽车需要感知周围环境，识别路标、行人、车辆等障碍物，从而实现自主导航。计算机视觉技术可以帮助自动驾驶汽车实现这些功能。例如，通过图像处理和识别算法，自动驾驶汽车能够准确地检测和识别路标、红绿灯和行人，从而做出相应的驾驶决策。
​	在工业制造领域，计算机视觉也发挥了重要作用。通过对生产线上的产品进行检测和分类，计算机视觉技术能够确保产品的质量和一致性。这种技术的应用不仅提高了生产效率，也降低了人工检测的误差率。
​	在医学领域，计算机视觉的应用更是不可或缺。通过对医学影像进行分析和诊断，计算机视觉技术能够帮助医生提高诊断准确性和效率。例如，通过分析 CT 扫描图像，医生能够更准确地诊断肿瘤等疾病。此外，计算机视觉还可以用于远程医疗、手术[机器人](https://cloud.baidu.com/product/abc-robot.html)等领域，为医疗行业的发展提供了强大的技术支持。
​	在安防监控领域，计算机视觉技术的应用也日益广泛。通过对[视频](https://cloud.baidu.com/product/mct.html)监控画面进行分析和识别，计算机视觉技术能够发现异常情况并报警，为公共[安全](https://cloud.baidu.com/solution/security/soc.html)提供了有力保障。例如，通过[人脸识别技术](https://cloud.baidu.com/product/face)，监控系统能够实时监测进出安全区域的人员，有效预防犯罪行为的发生。
​	除了以上应用领域，计算机视觉还在金融、[教育](https://cloud.baidu.com/solution/vr/education.html)、文化娱乐等领域发挥着重要作用。例如，在金融行业，计算机视觉技术可用于身份验证、反欺诈等场景；在教育行业，计算机视觉技术可以帮助教师进行教学评估和学生的学习效果评估；在文化娱乐行业，计算机视觉技术可以为电影、[游戏](https://cloud.baidu.com/solution/game.html)等提供更加丰富的视觉效果。
​	总之，计算机视觉技术的应用正在不断扩展和深化，它已经成为我们生活中不可或缺的一部分。



（2）同时，计算机视觉也带来了一些问题，如隐私问题、算法不公平性、伦理考量等。因此，社会需要制定适当的法规和伦理准则，以确保计算机视觉技术的应用是负责任和公正的。

**隐私与伦理：**

**事件1：“监测头环”进校园惹争议**

​	2019年11月，浙江一小学戴监控头环的视频引起广泛的关注与争议。在视频中，孩子们头上戴着号称“脑机接口”的头环，这些头环宣称可以记录孩子们上课时的专注程度，生成数据与分数发送给老师和家长。

​	对此，头环开发者回复，脑机接口技术是一种新兴技术，报道中提到的“打分”，是班级平均专注力数值，而非网友猜测的每个学生专注力数值。但有不少网友认为此头环是现代版的“头悬梁锥刺股”，会让学生产生逆反心理，并担忧是否涉及侵犯未成年人隐私。



**事件2：人脸识别第一案件—浙江教授状告杭州野生动物世界**

​	2019年，浙江理工大学特聘副教授郭兵购买了杭州野生动物世界年卡，支付了年卡卡费1360元。合同中承诺，持卡者可在该卡有效期一年内通过同时验证年卡及指纹入园，可在该年度不限次数畅游。

​	同年10月17日，杭州野生动物世界通过短信的方式告知郭兵“园区年卡系统已升级为人脸识别入园，原指纹识别已取消，未注册人脸识别的用户将无法正常入园，同时也无法办理退费”。郭兵认为，人脸识别等个人生物识别信息属于个人敏感信息，一旦泄露、非法提供或者滥用，将极易危害消费者人身和财产安全。协商无果后，郭兵于2019年10月28日向杭州市富阳区人民法院提起了诉讼，目前杭州市富阳区人民法院已正式受理此案。



**讨论：**

​	一方面，应当鼓励人脸识别等技术在十分必要的场景进行合理的使用，例如在追踪嫌疑人和寻找失联儿童等类似应用中。另一方面应当指出，人脸识别存在的潜在风险，如潜在的性别偏见、种族偏见、在对抗攻击面前表现出的脆弱性和安全隐患等，广泛的存在于其他类型的生物特征识别技术中，例如步态识别、指纹识别、虹膜识别、甚至是声纹识别。禁用某一项或某几项技术并不能规避可能的潜在风险，而是应采用技术和非技术手段改善现有技术存在的问题，积极避免可能存在的风险，并善用相关技术为人类造福。

**算法不公平性**：

**算法歧视不容忽视**

​	互联网上的算法歧视早已有之。算法歧视并不鲜见。图像识别软件犯过种族主义大错，比如，谷歌公司的图片软件曾错将黑人的照片标记为“大猩猩”， Flickr的自动标记系统亦曾错将黑人的照片标记为“猿猴”或者“动物”。 2016年3月23日，微软公司的人工智能聊天机器人Tay上线。出乎意料的是，Tay一开始和网民聊天，就被“教坏”了，成为了一个集反犹太人、性别歧视、种族歧视等于一身的“不良少女”。于是，上线不到一天，Tay就被微软公司紧急下线了。

​	互联网上的算法歧视问题早已引起人们注意。研究表明，在谷歌搜索中，相比搜索白人的名字，搜索黑人的名字更容易出现暗示具有犯罪历史的广告；在谷歌的广告服务中，男性比女性看到更多高薪招聘广告，当然，这可能和在线广告市场中固有的歧视问题有关，广告主可能更希望将特定广告投放给特定人群。此外，非营利组织ProPublica研究发现，虽然亚马逊公司宣称其“致力于成为地球上最以消费者为中心的公司”，但其购物推荐系统却一直偏袒其自己以及其合作伙伴的商品，即使其他卖家的商品的价格更低，而且，在其购物比价服务中，亚马逊公司隐瞒了其自己以及其合作伙伴的商品的运费，导致消费者不能得到公正的比价结果。

​	当人工智能用在应聘者评估上，可能引发雇佣歧视。如今，在医疗方面，人工智能可以在病症出现前几个月甚至几年就可以预测到病症的发生。当人工智能在对应聘者进行评估时，如果可以预测到该应聘者未来将会怀孕或者患上抑郁症，并将其排除在外，这将造成严重的雇佣歧视。伊隆•马斯克警告道，对于人工智能，如果发展不当，可能就是在“召唤恶魔”。当把包括道德决策在内的越来越多的决策工作委托给算法和人工智能，人们不得不深思，算法和人工智能未来会不会成为人的自由意志的主宰，会不会成为人类道德准则的最终发言人。





## 2. 未来的发展趋势和挑战

### **预计2024 年计算机视觉发展趋势：**

1. **可解释的 AI (XAI)：**
   随着对 AI 驱动系统依赖性的增加，计算机视觉模型的透明性和可解释性需求变得至关重要。可解释的 AI (XAI) 趋势正在获得关注，研究人员和从业人员专注于开发能够为其预测提供人类可理解的解释的模型，从而培养信任和问责制。
2. **边缘计算实时处理：**
   自动驾驶汽车、监视和增强现实等应用中对实时处理的需求推动了计算机视觉中边缘计算的采用。通过将处理过程置于更靠近数据源的位置，边缘计算可降低延迟、提高效率并促进更快的决策。
3. **三维计算机视觉的进步：**
   传统的计算机视觉主要在二维空间中运行，但未来在于三维感知。理解和交互环境的三维方面可以为机器人技术、医疗保健和虚拟现实等领域开辟新可能。预计三维对象识别和重建等技术将取得重大进展。
4. **跨领域学习：**
   计算机视觉系统变得更加通用，能够将知识跨不同领域进行转移。针对特定任务训练的模型可以利用其学习到的特征来执行不同领域中相关任务，从而形成更高效和更具适应性的系统。
5. **伦理和隐私考量：**
   随着计算机视觉应用在生活的各个方面的日益普及，伦理和隐私问题已经成为关注的焦点。在技术进步与保护个人隐私之间取得平衡是该领域必须在 2024 年解决的挑战。计算机视觉技术的负责任开发和部署对于其长期接受和成功至关重要。

### **克服挑战：**

虽然计算机视觉的发展轨迹充满希望，但仍需解决一些挑战：

1. **数据偏差：**
   训练数据中存在的偏差可能导致结果偏颇和不公平的结果。2024 年的努力应集中在构建更多元化和有代表性的数据集，以确保计算机视觉模型跨不同人口统计群体进行良好泛化。
2. **对对抗攻击的鲁棒性：**
   对抗攻击（对输入数据进行轻微修改可误导计算机视觉模型）仍然是一个问题。研究人员正在积极致力于开发能够抵御此类攻击的鲁棒模型，确保计算机视觉系统在现实世界场景中的可靠性。
3. **互操作性和标准化：**
   随着该领域扩展，在不同的计算机视觉框架和工具之间实现互操作性和标准化至关重要。这可以确保不同技术的无缝协作和集成。
4. **能源效率：**
   深度学习模型资源密集的特性对能耗提出了挑战。研究工作重点放在开发更高能效的算法和硬件架构，以使计算机视觉应用可持续发展。
5. **法律和监管框架：**
   随着计算机视觉应用在社会中根深蒂固，对明确的法律和监管框架的需求变得显而易见。解决与问责制、责任和数据所有权相关的问题，对于负责任地和合乎道德地部署这些技术至关重要。

## 3. 鼓励创新和研究的重要性

​	计算机视觉领域的研究主题涉及广泛，包括但不限于图像识别、目标检测、人脸识别、视觉推理、三维重建等。这些研究主题在各个领域都产生了深远的影响，体现了计算机视觉技术的创新力和应用潜力。

​	在图像识别方面，研究者通过深度学习等技术不断提高算法的准确性，使计算机能够自动识别和理解图像中的对象。这有助于提高医学影像的分析效率、改善交通监控系统的性能，并在各种应用场景中实现更精确的图像分类和识别。

​	目标检测是计算机视觉中一个重要的研究方向，关注如何在图像或视频中准确地定位和识别特定目标。这对于自动驾驶、安防监控和工业自动化等领域具有重要意义，提高了系统的感知能力和决策效果。

​	人脸识别技术的不断创新推动了安防领域和生物识别系统的发展。通过深度学习和人工智能技术，人脸识别系统变得更加准确和稳健，被广泛应用于身份验证、公共安全和社交媒体等领域。

​	视觉推理方面的研究致力于使计算机系统能够更好地理解和推断场景中的语境，从而提高系统对复杂情境的理解能力。这有助于改善自动驾驶系统对于复杂交通环境的应对能力，以及改进虚拟助手和智能家居系统的用户交互体验。

​	三维重建是通过计算机视觉技术还原三维空间的形状和结构。这在工业设计、建筑、文化遗产保护等领域有着广泛的应用，为数字化和虚拟现实技术提供了基础。

​	总体而言，计算机视觉的研究主题涵盖了多个领域，其不断创新推动了技术的发展和应用的拓展。这些研究不仅为科技进步和社会发展提供了新的解决方案，同时也强调了鼓励创新和研究的重要性。通过不断追求新的理念、算法和应用，计算机视觉研究为社会带来了更智能、更高效的解决方案，凸显了创新与研究在推动科技和社会进步中的关键作用。